# Code Understanding Orchestrator Service

> ğŸ§‘â€ğŸ³ The **Sous Chef** in the Kitchen Brigade Architecture

A standalone microservice that coordinates multiple specialized code understanding models to dynamically extract, validate, and rank search terms from natural language queries.

## ğŸ¯ Purpose

This service solves the **false positive problem** in cross-reference systems. Instead of hardcoded keyword mappings that match "chunk" to C++ memory allocation, it uses AI models to understand context and extract semantically relevant terms.

**Before:**
```python
# Hardcoded, brittle
FOCUS_SEARCH_TERMS = {
    "chunking": ["chunk", "split", "segment"]  # Matches C++ memory allocation!
}
```

**After:**
```python
# Dynamic, context-aware
response = orchestrator.extract(
    query="LLM document chunking with overlap",
    domain="ai-ml"
)
# Returns: ["chunking", "RAG", "text_splitter", "embedding"] âœ…
# Excludes: ["chunk of memory", "memory allocation"] âœ…
```

## ğŸ—ï¸ Architecture

```
Customer (Claude/GPT) 
    â†“
Sous Chef (This Service) â† Extracts keywords, curates results
    â†“
Cookbook (Semantic Search) â† Dumb retrieval
    â†“
Sous Chef (Curation Phase) â† Filters irrelevant results
    â†“
Line Cook (Code Generator) â† Generates code
    â†“
Customer receives working code
```

See [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) for full details.

## ğŸ§  Models Used

### Keyword Extraction Pipeline
| Model | Role | HuggingFace ID |
|-------|------|----------------|
| **CodeT5+** | Generator | `Salesforce/codet5p-220m` |
| **GraphCodeBERT** | Validator | `microsoft/graphcodebert-base` |
| **CodeBERT** | Ranker | `microsoft/codebert-base` |

### Code Generation (Line Cook)
| Model | Parameters | Best For |
|-------|------------|----------|
| **Qwen2.5-Coder-32B** | 32B | Primary - Production |
| **Qwen2.5-Coder-7B** | 7.6B | Fallback - Development |

### Semantic Similarity (SBERT)
| Model | Dimensions | Best For |
|-------|------------|----------|
| **all-MiniLM-L6-v2** | 384 | Semantic embeddings, similar chapters |

## ğŸ“¦ Dependencies

### Required Dependencies
Core packages that must be installed for the service to function:

| Package | Version | Purpose |
|---------|---------|---------|
| `fastapi` | >=0.109.0 | REST API framework |
| `transformers` | >=4.36.0 | HuggingFace model loading |
| `torch` | >=2.1.0 | PyTorch backend |
| `sentence-transformers` | >=2.2.2 | SBERT semantic embeddings |
| `scikit-learn` | ~=1.3.0 | TF-IDF fallback, cosine similarity |
| `httpx` | >=0.26.0 | HTTP client (connection pooling per anti-pattern #12) |

### Optional Dependencies (Graceful Degradation)
The service implements a three-tier fallback for semantic similarity:

1. **SBERT** (Primary): Uses `sentence-transformers` for high-quality 384-dim embeddings
2. **TF-IDF** (Fallback): Uses `scikit-learn` when SBERT model unavailable
3. **Service Error** (Last Resort): Returns error response, service stays up

This ensures the service remains operational even if the SBERT model fails to load.

```python
# Example: Check which mode is active
from src.models.sbert.semantic_similarity_engine import SemanticSimilarityEngine

engine = SemanticSimilarityEngine()
if engine.is_using_fallback:
    print("Running in TF-IDF fallback mode")
else:
    print("Running with SBERT embeddings")
```

## ğŸš€ Quick Start

```bash
# Clone
git clone https://github.com/kevin-toles/Code-Orchestrator-Service.git
cd Code-Orchestrator-Service

# Install dependencies
pip install -r requirements.txt

# Download models (first run)
python scripts/download_models.py

# Run
uvicorn src.main:app --reload --port 8080
```

## ğŸ“¡ API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/v1/extract` | POST | Extract search terms from query |
| `/api/v1/validate` | POST | Validate terms against domain |
| `/api/v1/search` | POST | Full pipeline: extract + search |
| `/api/v1/generate` | POST | Generate code from context |
| `/health` | GET | Health check |

### Example Request

```bash
curl -X POST http://localhost:8080/api/v1/extract \
  -H "Content-Type: application/json" \
  -d '{
    "query": "LLM document chunking with overlap for RAG",
    "domain": "ai-ml",
    "options": {
      "min_confidence": 0.7,
      "max_terms": 10
    }
  }'
```

### Example Response

```json
{
  "search_terms": [
    {"term": "chunking", "score": 0.95, "models_agreed": 3},
    {"term": "RAG", "score": 0.92, "models_agreed": 3},
    {"term": "embedding", "score": 0.89, "models_agreed": 3},
    {"term": "text_splitter", "score": 0.85, "models_agreed": 2}
  ],
  "excluded_terms": [
    {"term": "split", "reason": "Too generic", "models_agreed": 1}
  ],
  "metadata": {
    "processing_time_ms": 245,
    "models_used": ["codet5", "graphcodebert", "codebert"]
  }
}
```

## ğŸ“ Project Structure

```
Code-Orchestrator-Service/
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                 # FastAPI entry point
â”‚   â”œâ”€â”€ api/                    # REST endpoints
â”‚   â”œâ”€â”€ agents/                 # Model agents (CodeT5+, etc.)
â”‚   â”œâ”€â”€ orchestrator/           # LangGraph state machine
â”‚   â”œâ”€â”€ models/                 # Model loading/inference
â”‚   â””â”€â”€ config/                 # Settings
â”‚
â”œâ”€â”€ tests/
â”œâ”€â”€ scripts/
â””â”€â”€ docs/
    â””â”€â”€ ARCHITECTURE.md         # Full architecture docs
```

## ğŸ”— Related Services

| Service | Purpose |
|---------|---------|
| [semantic-search-service](../semantic-search-service) | Vector DB queries (Cookbook) |
| [llm-gateway](../llm-gateway) | Model routing |
| [ai-agents](../ai-agents) | Main orchestration |

## ğŸ“‹ Development Status

- [ ] Phase 1: Basic FastAPI structure
- [ ] Phase 2: CodeT5+ generator agent
- [ ] Phase 3: GraphCodeBERT validator agent
- [ ] Phase 4: CodeBERT ranker agent
- [ ] Phase 5: LangGraph orchestration
- [ ] Phase 6: Line Cook integration
- [ ] Phase 7: Integration tests
- [ ] Phase 8: Docker deployment

## ğŸ“„ License

MIT
